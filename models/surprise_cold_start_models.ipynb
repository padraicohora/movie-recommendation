{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "from surprise import BaselineOnly, Dataset, Reader, SVD, NMF, SVDpp, accuracy, PredictionImpossible, KNNWithMeans, KNNBasic, NormalPredictor, KNNWithZScore, KNNBaseline, SlopeOne, CoClustering\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV, PredefinedKFold\n",
    "from surprise.model_selection.split import LeaveOneOut\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "genre_cols = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', \n",
    "              'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', \n",
    "              'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "column_names = [\"item\",\"title\",\"genres\",\"movie_name\",\"movie_year\",\"(no genres listed)\",\"Action\",\"Adventure\",\"Animation\",\"Children\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\"Horror\",\"IMAX\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\",\"user\",\"rating\",\"rating_timestamp\",\"rating_year\",\"rating_month\",\"rating_season,tag\",\"tag_timestamp\",\"cleaned_tag\",\"tag_length\",\"tag_year\"]\n",
    "\n",
    "data = pd.read_csv('../samples/combined_movies_ratings_tags.csv', names=column_names, index_col=False, skiprows=0)\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "ratings = data[['user', 'item', 'rating']]\n",
    "ratings = ratings.iloc[1:]\n",
    "ratings[\"rating\"] = ratings[\"rating\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial testing creating a dataset of 10k users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = Dataset.load_from_df(ratings, reader)\n",
    "\n",
    "print(custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "trainset, testset = train_test_split(custom_data, test_size=0.2)\n",
    "\n",
    "# Convert trainset to dataframe (for content-based model)\n",
    "train_df = pd.DataFrame(trainset.build_testset(), columns=['user', 'item', 'rating'])\n",
    "test_df = pd.DataFrame(testset, columns=['user', 'item', 'rating'])\n",
    "\n",
    "\n",
    "# Step 1: Filter users with >= 5 test ratings\n",
    "test_user_counts = test_df['user'].value_counts()\n",
    "eligible_users = test_user_counts[test_user_counts >= 5].index.tolist()\n",
    "\n",
    "# Different number of known ratings to test\n",
    "known_ratings_list = [5]\n",
    "# known_ratings_list = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the best params for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measures=[\"rmse\", \"mae\", \"mse\"]\n",
    "\n",
    "def perform_grid_search(algo, params):\n",
    "    \n",
    "    gs = GridSearchCV(algo, params, measures=measures, cv=3, n_jobs=-1, joblib_verbose=0)\n",
    "\n",
    "    gs.fit(custom_data)\n",
    "\n",
    "    \n",
    "    # best RMSE score\n",
    "    print(gs.best_score)\n",
    "\n",
    "\n",
    "    # combination of parameters that gave the best measure score\n",
    "    print(gs.best_params)\n",
    "    return gs.best_estimator[\"rmse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to run the cold start train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cold_start_models(models, model_names, train_df, test_df, eligible_users, known_ratings_list, k=10):\n",
    "    all_results = []\n",
    "\n",
    "    for algo, model_name in zip(models, model_names):\n",
    "        rmse_results = []\n",
    "        mae_results = []\n",
    "        mse_results = []\n",
    "        ndcg_results = []\n",
    "\n",
    "        # Iterate over each number of known ratings\n",
    "        for known_ratings in known_ratings_list:\n",
    "            print(f\"Processing for {known_ratings} known ratings per user with {model_name}...\")\n",
    "            \n",
    "            # Step 3: Reduce training data to 'known_ratings' ratings per user for these test users (simulate cold start)\n",
    "            limited_train_rows = []\n",
    "            for user in eligible_users:\n",
    "                user_ratings = train_df[train_df['user'] == user]\n",
    "                if len(user_ratings) > known_ratings:\n",
    "                    sampled = user_ratings.sample(known_ratings, random_state=42)\n",
    "                else:\n",
    "                    sampled = user_ratings\n",
    "                limited_train_rows.append(sampled)\n",
    "\n",
    "            # Step 4: Add all training data from non-eligible users (normal users)\n",
    "            non_eligible_users_df = train_df[~train_df['user'].isin(eligible_users)]\n",
    "            cold_start_train_df = pd.concat(limited_train_rows + [non_eligible_users_df], ignore_index=True)\n",
    "\n",
    "            # Build training set for Surprise\n",
    "            reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "            cold_start_data = Dataset.load_from_df(cold_start_train_df[['user', 'item', 'rating']], reader)\n",
    "            cold_start_trainset = cold_start_data.build_full_trainset()\n",
    "\n",
    "            # Train the model\n",
    "            algo.fit(cold_start_trainset)\n",
    "\n",
    "            # Build the final test set for Surprise\n",
    "            final_testset = [tuple(x) for x in test_df.to_numpy()]\n",
    "            predictions = algo.test(final_testset)\n",
    "\n",
    "            # Evaluate\n",
    "            rmse = accuracy.rmse(predictions, verbose=False)\n",
    "            mae = accuracy.mae(predictions, verbose=False)\n",
    "            mse = accuracy.mse(predictions, verbose=False)\n",
    "            \n",
    "            rmse_results.append((known_ratings, rmse))\n",
    "            mae_results.append((known_ratings, mae))\n",
    "            mse_results.append((known_ratings, mse))\n",
    "            \n",
    "            print(f\"RMSE on filtered cold-start test users (with {known_ratings} training ratings each) for {model_name}: {rmse}\")\n",
    "            print(f\"MAE on filtered cold-start test users (with {known_ratings} training ratings each) for {model_name}: {mae}\")\n",
    "            print(f\"MSE on filtered cold-start test users (with {known_ratings} training ratings each) for {model_name}: {mse}\")\n",
    "\n",
    "            # Calculate NDCG@K\n",
    "            # Calculate NDCG@K\n",
    "            ndcg_scores = []\n",
    "            for user_id in eligible_users:\n",
    "                user_ratings = train_df[train_df['user'] == user_id]\n",
    "                \n",
    "                # Predict ratings for unseen movies\n",
    "                all_movies = set(train_df['item'].unique())\n",
    "                seen_movies = set(user_ratings['item'])\n",
    "                unseen_movies = list(all_movies - seen_movies)\n",
    "                \n",
    "                # Filter test data for the current user\n",
    "                user_test_ratings = test_df[test_df['user'] == user_id]\n",
    "                if user_test_ratings.empty:\n",
    "                    continue  # Skip user if no test data\n",
    "                \n",
    "                predictions = [ (movie_id, algo.predict(user_id, movie_id).est) for movie_id in unseen_movies ]\n",
    "                predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Top-K recommended movies\n",
    "                top_k_movies = [movie_id for movie_id, _ in predictions[:k]]\n",
    "                \n",
    "                # Movies the user actually liked (rating >= 4) in the test set\n",
    "                true_liked_movies = user_test_ratings[user_test_ratings['rating'] >= 4]['item'].tolist()\n",
    "                \n",
    "                if not true_liked_movies:\n",
    "                    continue  # Skip user if no strong likes in the test set\n",
    "                \n",
    "                # --- Calculate NDCG@K ---\n",
    "                dcg = 0.0\n",
    "                for idx, movie_id in enumerate(top_k_movies):\n",
    "                    if movie_id in true_liked_movies:\n",
    "                        dcg += 1.0 / np.log2(idx + 2)  # +2 because index starts from 0\n",
    "                \n",
    "                # Ideal DCG (IDCG)\n",
    "                ideal_relevant = min(len(true_liked_movies), k)\n",
    "                idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_relevant))\n",
    "                \n",
    "                ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "                ndcg_scores.append(ndcg)\n",
    "\n",
    "            average_ndcg = np.mean(ndcg_scores)\n",
    "            ndcg_results.append((known_ratings, average_ndcg))\n",
    "            print(f\"NDCG@{k} on filtered cold-start test users (with {known_ratings} training ratings each) for {model_name}: {average_ndcg}\")\n",
    "\n",
    "        # Create a DataFrame to store the results for the current model\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': model_name,\n",
    "            'Known Ratings': [r[0] for r in rmse_results],\n",
    "            'RMSE': [r[1] for r in rmse_results],\n",
    "            'MAE': [r[1] for r in mae_results],\n",
    "            'MSE': [r[1] for r in mse_results],\n",
    "            'NDCG@K': [r[1] for r in ndcg_results]\n",
    "        })\n",
    "\n",
    "        # Append the results to the all_results list\n",
    "        all_results.append(results_df)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    return final_results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grid search for best params<h3>\n",
    "\n",
    "First create a reusable function to run grid search for any model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>SVD</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svd = {\n",
    "    'n_factors': [20, 50, 100, 150, 200],\n",
    "    'n_epochs': [20, 30, 40, 50, 60],\n",
    "    'lr_all': [0.002, 0.005, 0.007, 0.01, 0.02],\n",
    "    'reg_all': [0.02, 0.05, 0.1, 0.15, 0.2]\n",
    "}\n",
    "\n",
    "best_svd = perform_grid_search(SVD, param_grid_svd)\n",
    "\n",
    "models += [best_svd]\n",
    "model_names += ['SVD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>SVD++</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svdpp = {\n",
    "    'n_factors': [20, 50, 100, 150, 200],\n",
    "    'n_epochs': [10, 20, 30, 40, 50],\n",
    "    'lr_all': [0.002, 0.005, 0.007, 0.01, 0.02],\n",
    "    'reg_all': [0.02, 0.05, 0.1, 0.15, 0.2]\n",
    "}\n",
    "\n",
    "best_svdpp = perform_grid_search(SVDpp, param_grid_svdpp)\n",
    "\n",
    "models += [best_svdpp]\n",
    "model_names += ['SVD++']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NMF</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nmf = {\n",
    "    'n_factors': [15, 30, 50, 100, 150],\n",
    "    'n_epochs': [50, 100, 150, 200, 300],\n",
    "    'reg_pu': [0.06, 0.08, 0.1, 0.2, 0.3],\n",
    "    'reg_qi': [0.06, 0.08, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "best_nmf = perform_grid_search(NMF, param_grid_nmf)\n",
    "\n",
    "models += [best_nmf]\n",
    "model_names += ['NMF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>KNN basic</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn_basic = {\n",
    "    'k': [10, 20, 30, 40, 50],\n",
    "    'min_k': [1, 3, 5, 7, 10],\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson', 'pearson_baseline'],\n",
    "        'user_based': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_knn_basic = perform_grid_search(KNNBasic, param_grid_knn_basic)\n",
    "\n",
    "models += [best_knn_basic]\n",
    "model_names += ['KNNBasic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>KNN with means</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn_means = {\n",
    "    'k': [10, 20, 30, 40, 50],\n",
    "    'min_k': [1, 3, 5, 7, 10],\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson_baseline'],\n",
    "        'user_based': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_knn_with_means = perform_grid_search(KNNWithMeans, param_grid_knn_means)\n",
    "\n",
    "models += [best_knn_with_means]\n",
    "model_names += ['KNNWithMeans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>KNN with Z Score</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn_zscore = {\n",
    "    'k': [10, 20, 30, 40, 50],\n",
    "    'min_k': [1, 3, 5, 7, 10],\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson'],\n",
    "        'user_based': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_knn_z_score = perform_grid_search(KNNWithZScore, param_grid_knn_zscore)\n",
    "\n",
    "models += [best_knn_z_score]\n",
    "model_names += ['KNNWithZScore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>KNN with Baseline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn_baseline = {\n",
    "    'k': [10, 20, 30, 40, 50],\n",
    "    'min_k': [1, 3, 5, 7, 10],\n",
    "    'sim_options': {\n",
    "        'name': ['pearson_baseline', 'cosine'],\n",
    "        'user_based': [True, False]\n",
    "    },\n",
    "    'bsl_options': {\n",
    "        'method': ['sgd', 'als'],\n",
    "        'reg': [0.01, 0.02, 0.05, 0.1],\n",
    "        'learning_rate': [0.002, 0.005, 0.01]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_knn_baseline = perform_grid_search(KNNBaseline, param_grid_knn_baseline)\n",
    "\n",
    "models += [best_knn_baseline]\n",
    "model_names += ['KNNBaseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>CoClustering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_coclustering = {\n",
    "    'n_cltr_u': [3, 5, 10, 15, 20],\n",
    "    'n_cltr_i': [3, 5, 10, 15, 20],\n",
    "    'n_epochs': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "best_coclustering = perform_grid_search(CoClustering, param_grid_coclustering)\n",
    "\n",
    "models += [best_coclustering]\n",
    "model_names += ['CoClustering']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Baseline Only</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_baseline_only = {\n",
    "    'bsl_options': {\n",
    "        'method': ['sgd', 'als'],\n",
    "        'reg': [0.01, 0.02, 0.05, 0.1],\n",
    "        'learning_rate': [0.002, 0.005, 0.01],\n",
    "        'n_epochs': [5, 10, 20, 30, 40]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_baseline_only = perform_grid_search(BaselineOnly, param_grid_baseline_only)\n",
    "\n",
    "models += [best_baseline_only]\n",
    "model_names += ['BaselineOnly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Models with no parameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopeone_algo = SlopeOne()\n",
    "normalPredictor_algo = NormalPredictor()\n",
    "\n",
    "\n",
    "models += [slopeone_algo, normalPredictor_algo]\n",
    "model_names += ['SlopeOne', \"NormalPredictor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run best models with cold start data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = run_cold_start_models(models, model_names, train_df, test_df, eligible_users, known_ratings_list)\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# results_df = run_cold_start_models(models, model_names)\n",
    "# print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_recommender_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
